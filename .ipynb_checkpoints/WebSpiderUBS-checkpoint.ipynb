{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebSpiderUBS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqh27s71HA4K"
      },
      "source": [
        "# This is the Notebook I have used to create the N level web spider for UBS Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlJAwVlAGzpm"
      },
      "source": [
        "## Importing All Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZQRzayfZFQK",
        "outputId": "be8567a5-f220-49fb-b080-9568add9c51c"
      },
      "source": [
        "pip install tornado==4.5.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tornado==4.5.3 in /usr/local/lib/python3.6/dist-packages (4.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jePmu2OGw7t",
        "outputId": "427183d7-ff73-4ac8-e426-99cd66118afe"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "import time\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import asyncio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9rjFkSZHQxF"
      },
      "source": [
        "## The function for getting links from the soup created by Beautiful Soup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEIziMOoHadO"
      },
      "source": [
        "'''\n",
        "This function is used for getting all links from the Soup \n",
        "It is basically a nested function. The getLink(e) function takes a link and makes it a valid URL, if it is not already valid\n",
        "The getLink function has all the possible types of URLs that could be gotten from the soup object. It makes them traversabale\n",
        "'''\n",
        "def getLinksFromSoup(baseURL, soup): \n",
        "    def getLink(e):\n",
        "        link = e[\"href\"]\n",
        "        if len(link) < 1:\n",
        "            return ''\n",
        "        if link.startswith('//'): \n",
        "            return 'http:'+link\n",
        "        if link.startswith('?'):\n",
        "            if baseURL.endswith('/'):\n",
        "                return baseURL[:-1]+link\n",
        "            else:\n",
        "                return baseURL+link\n",
        "        if link[0] == '/':\n",
        "            if baseURL.endswith(link):\n",
        "                return ''\n",
        "            if baseURL[-1] != '/':\n",
        "                return baseURL+link\n",
        "            else:\n",
        "                return baseURL+link[1:]\n",
        "        elif link[0] == '#':\n",
        "            return ''\n",
        "        elif len(link) > 7:\n",
        "            return link\n",
        "        else:\n",
        "            return ''\n",
        "    '''\n",
        "    It maps each link from the soup to the getLink function. After that the getLink function sends back valid URLs one by one\n",
        "    '''        \n",
        "    allLinks = list(map(getLink, soup.find_all('a', href=True))) \n",
        "    allLinks = [link for link in allLinks if link] #removing empty links from the links gotten from a page\n",
        "    return list(set(allLinks)) #remove duplicates and return a list of links"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yPxEmJiHfMF"
      },
      "source": [
        "## The function for getting all Words from the soup created by Beautiful Soup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SpfFGIaHls1"
      },
      "source": [
        "blacklist = ['[document]', 'noscript', 'header',\n",
        "             'html', 'meta', 'head', 'script', 'style']\n",
        "\n",
        "\n",
        "'''\n",
        "This function is used for getting all words from the soup\n",
        "'''\n",
        "def getWordsFromSoup(soup):\n",
        "    text = soup.find_all(text=True) #finding out all the text from the soup\n",
        "    output = ''\n",
        "    outputSentences = []\n",
        "    '''\n",
        "    Cleaning the text received from the soup. Firstly we will remove all elements having their parents in blacklist \n",
        "    '''\n",
        "    for t in text:\n",
        "        if t.parent.name not in blacklist:\n",
        "            output += '{} '.format(t)\n",
        "            outputSentences.append('{} '.format(t))\n",
        "    outputSentences = [i.strip() for i in outputSentences]\n",
        "    # removing special characters\n",
        "    outputSentences = [re.sub('[^a-zA-Z0-9]+', ' ', _)\n",
        "                       for _ in outputSentences]\n",
        "    # removing all only digit phrases       \n",
        "    outputSentences = [' '.join(s for s in i.split() if not any(\n",
        "        c.isdigit() for c in s)) for i in outputSentences]\n",
        "    #removing empty sentences\n",
        "    outputSentences = [i for i in outputSentences if i]\n",
        "    #tokenising the sentence phrases using nltk\n",
        "    outputSentences = [nltk.tokenize.sent_tokenize(i) for i in outputSentences]\n",
        "    words = list(output.split(' '))\n",
        "    words = [re.sub('[^a-zA-Z0-9]+', ' ', _) for _ in words]\n",
        "    allWords = []\n",
        "    #Filtering words and removing words having length less than 2 and numbers\n",
        "    for word in words:\n",
        "        if len(word) > 2:\n",
        "            wordsInCurrent = word.split(' ')\n",
        "            for w in wordsInCurrent:\n",
        "                if len(w) > 2 and not(w.isdecimal()):\n",
        "                    allWords.append(w.lower()) #appending lowercase words to the final list\n",
        "    return allWords, outputSentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4daA76dQHraK"
      },
      "source": [
        "## The function for getting Bigrams from a list of phrases/sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAjyB_j3Hq9t"
      },
      "source": [
        "'''\n",
        "This function is used to get all bigrams(words coming together in pair) level by level from all the sentences and phrases we have gotten from the site\n",
        "'''\n",
        "def getBigrams(allSentences):\n",
        "    allBiagrams = []\n",
        "    for sentencesPerLevel in allSentences:\n",
        "        bigram = []\n",
        "        for sentence in sentencesPerLevel:\n",
        "            token = nltk.word_tokenize(sentence) #tokenising the phrase\n",
        "            token = [word.lower() for word in token if word not in stopwords.words('english')] #removing stopwords **only english\n",
        "            bi = list(ngrams(token, 2)) #creating bigrams using ngrams\n",
        "            [bigram.append(i[0]+\" \" + i[1]) for i in bi if i]\n",
        "        [allBiagrams.append(i) for i in bigram]\n",
        "    bigrams = {} #making a bigram frequency dictionary for plotting graphs\n",
        "    for i in allBiagrams:\n",
        "        bigrams[i] = bigrams.get(i, 0) + 1\n",
        "    b = Counter(bigrams) \n",
        "    #return all the bigrams sorted in descending order of frequency\n",
        "    return b.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7GVyivTH67i"
      },
      "source": [
        "## The function for analyzing words from a list containings lists of words in each level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM0T_xSPH7T_"
      },
      "source": [
        "'''\n",
        "This function is used to analyze the words level by level for plotting graphs.\n",
        "The statistics done are:\n",
        "1. Count of Words Per Level including stopwords\n",
        "2. Word Cloud with frequencies without stopwords\n",
        "3. Average Length of Words Per Level including stopwords\n",
        "'''\n",
        "def analyzeWords(allWords):\n",
        "    level = 1\n",
        "    words = {}\n",
        "    countOfWordsPerLevel = {}\n",
        "    averageLengthOfWordsPerLevel = {}\n",
        "    for wordsPerLevel in allWords:\n",
        "        countOfWordsPerLevel[\"Level \"+str(level)]=len(wordsPerLevel)\n",
        "        averageLengthOfWordsPerLevel[\"Level \"+str(level)]= sum(len(s) for s in wordsPerLevel)/len(wordsPerLevel)\n",
        "        level += 1\n",
        "        filtered_words = [\n",
        "            word for word in wordsPerLevel if word not in stopwords.words('english')] #filtering words using nltk by removing stopwords in english only\n",
        "        count = {}\n",
        "        for i in filtered_words:\n",
        "            count[i] = count.get(i, 0) + 1 #This dictionary can be used to analyze words individually per level\n",
        "            words[i] = words.get(i, 0)+1\n",
        "        c = Counter(count)\n",
        "    w = Counter(words) #making a counter of the words\n",
        "    return w.most_common(), countOfWordsPerLevel, averageLengthOfWordsPerLevel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdSpIsxhI0AJ"
      },
      "source": [
        "## The main scrapping function which starts gathering data starting from the base URL with specified depth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9cnm9INIwha"
      },
      "source": [
        "'''\n",
        "This is the main function for starting the scrapping. The parameters it takes are the baseURL and the maximum depth it has to go till.\n",
        "'''\n",
        "def startScrapingWithoutMultithreading(baseURL, maxLevels):\n",
        "    visited = {} # a visited dictionary to keep track of the visited links and also can be useful for counting how many times any URL occurs on a page\n",
        "    allURLs = [] #a list to maintain the URLs per level\n",
        "    allWords = [] #a list to maintain all the Words per level\n",
        "    allSentences = [] # a list to maintain all the sentences and phrases per level  \n",
        "    for level in range(0, maxLevels+1):\n",
        "        if level == 0:\n",
        "            visited[baseURL] = 1 #marking the URL visited so that it will not be visited again\n",
        "            response = requests.get(baseURL)#getting the page \n",
        "            html_page = response.text\n",
        "            soup = BeautifulSoup(html_page, 'lxml')\n",
        "\n",
        "            if response.status_code != 404:\n",
        "                links = getLinksFromSoup(baseURL, soup)\n",
        "                words, sentences = getWordsFromSoup(soup)#getting allWords and phrases/sentences from that webpage\n",
        "                #modifing and inserting in respective lists\n",
        "                allWords.append(words)\n",
        "                s = []\n",
        "                for sentence in sentences:\n",
        "                    s.append(sentence[0])\n",
        "                allSentences.append(list(s))\n",
        "                allURLs.append(baseURL.split())\n",
        "                allURLs.append(links)\n",
        "        elif level == maxLevels:\n",
        "             #if the level is maxLevel we will not find the links on these pages and just find the words and sentences.\n",
        "            wordsInCurrentLevel = np.array([])\n",
        "            sentencesInCurrentLevel = np.array([])\n",
        "            for link in allURLs[-1]:#traversing all URL's received from the previous level\n",
        "                #only visiting the unvisited URLs and not visiting mails, images, js, etc\n",
        "                if link not in visited.keys() and ((not link.startswith(\"mailto:\")) and (not (\"javascript:\" in link)) and (not link.endswith(\".png\")) and (not link.endswith(\".jpg\")) and (not link.endswith(\".jpeg\"))):\n",
        "                    response = requests.get(link)\n",
        "                    html_page = response.text\n",
        "                    soup = BeautifulSoup(html_page, 'lxml')\n",
        "                    words, sentences = getWordsFromSoup(soup)\n",
        "                    sentencesInCurrentLevel = np.append(\n",
        "                        sentencesInCurrentLevel, sentences)\n",
        "                    wordsInCurrentLevel = np.append(wordsInCurrentLevel, words)\n",
        "                    visited[link] = 1\n",
        "                else:\n",
        "                    if link in visited.keys():\n",
        "                        visited[link] += 1\n",
        "            allWords.append(list(wordsInCurrentLevel))\n",
        "            allSentences.append(list(sentencesInCurrentLevel))\n",
        "        else:\n",
        "            URLsInCurrentLevel = []\n",
        "            wordsInCurrentLevel = np.array([])\n",
        "            sentencesInCurrentLevel = np.array([])\n",
        "            for link in allURLs[-1]:\n",
        "                if link not in visited.keys() and ((not link.startswith(\"mailto:\")) and (not (\"javascript:\" in link)) and (not link.endswith(\".png\")) and (not link.endswith(\".jpg\")) and (not link.endswith(\".jpeg\"))):\n",
        "                    visited[link] = 1\n",
        "                    try:\n",
        "                        response = requests.get(link)\n",
        "                        html_page = response.text\n",
        "                        soup = BeautifulSoup(html_page, 'lxml')\n",
        "                    except:\n",
        "                        response.status_code = 404\n",
        "                    links = []\n",
        "                    if response.status_code != 404:\n",
        "                        links = getLinksFromSoup(link, soup)\n",
        "                        words, sentences = getWordsFromSoup(soup)\n",
        "                        sentencesInCurrentLevel = np.append(\n",
        "                            sentencesInCurrentLevel, sentences)\n",
        "                        wordsInCurrentLevel = np.append(\n",
        "                            wordsInCurrentLevel, words)\n",
        "                        [URLsInCurrentLevel.append(\n",
        "                            link) for link in links if link not in visited.keys()]\n",
        "            allURLs.append(URLsInCurrentLevel)\n",
        "            allWords.append(list(wordsInCurrentLevel))\n",
        "            allSentences.append(list(sentencesInCurrentLevel))\n",
        "\n",
        "     #analyzing the data gotten from crawling\n",
        "    wordCloudWords, countOfWordsPerLevel, averageLengthOfWordsPerLevel = analyzeWords(\n",
        "        allWords)\n",
        "    #getting bigrams from sentences and phrases    \n",
        "    allBiagrams = getBigrams(allSentences)\n",
        "    \n",
        "\n",
        "    wordCloud = {}\n",
        "    bigramCloud = {}\n",
        "    for key, val in wordCloudWords:\n",
        "        wordCloud[key]=val\n",
        "        # print(wordCloud)\n",
        "    for key, val in allBiagrams:\n",
        "        bigramCloud[key]=val\n",
        "    return (wordCloud, countOfWordsPerLevel,  averageLengthOfWordsPerLevel, bigramCloud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXVvcHHvTjkG"
      },
      "source": [
        "'''\n",
        "This function is used for getting all links from the Soup \n",
        "It is basically a nested function. The getLink(e) function takes a link and makes it a valid URL, if it is not already valid\n",
        "The getLink function has all the possible types of URLs that could be gotten from the soup object. It makes them traversabale\n",
        "'''\n",
        "def getLinksFromLinkAndSession(baseURL, session): \n",
        "    def getLink(e):\n",
        "        link = e[\"href\"]\n",
        "        if len(link) < 1:\n",
        "            return ''\n",
        "        if link.startswith('//'): \n",
        "            return 'http:'+link\n",
        "        if link.startswith('?'):\n",
        "            if baseURL.endswith('/'):\n",
        "                return baseURL[:-1]+link\n",
        "            else:\n",
        "                return baseURL+link\n",
        "        if link[0] == '/':\n",
        "            if baseURL.endswith(link):\n",
        "                return ''\n",
        "            if baseURL[-1] != '/':\n",
        "                return baseURL+link\n",
        "            else:\n",
        "                return baseURL+link[1:]\n",
        "        elif link[0] == '#':\n",
        "            return ''\n",
        "        elif len(link) > 7:\n",
        "            return link\n",
        "        else:\n",
        "            return ''\n",
        "    '''\n",
        "    It maps each link from the soup to the getLink function. After that the getLink function sends back valid URLs one by one\n",
        "    '''     \n",
        "    try:   \n",
        "        response=session.get(baseURL) #getting the page\n",
        "    except:\n",
        "        return []     \n",
        "    if response.ok:\n",
        "        html_page = response.text #extracting text from the response\n",
        "        soup = BeautifulSoup(html_page, 'lxml') #Creating a soup using the lxml parser\n",
        "        allLinks = list(map(getLink, soup.find_all('a', href=True))) \n",
        "        allLinks = [link for link in allLinks if link] #removing empty links from the links gotten from a page\n",
        "        return list(set(allLinks)) #remove duplicates and return a list of links\n",
        "    else:    \n",
        "        return []    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdnvwgpNToKR"
      },
      "source": [
        "# Include the list of parents to blacklist, user can change it as per his will\n",
        "blacklist = ['[document]', 'noscript', 'header',\n",
        "             'html', 'meta','head', 'script', 'style']\n",
        "'''\n",
        "This function is used for getting all words from the soup\n",
        "'''\n",
        "def getWordsAndSentencesFromLinkAndSession(link, session):\n",
        "    try:\n",
        "        response=session.get(link) #getting the page\n",
        "    except:\n",
        "        return [],[]     \n",
        "    if response.ok:\n",
        "        html_page = response.text #extracting text from the response\n",
        "        soup = BeautifulSoup(html_page, 'lxml') #Creating a soup using the lxml parser\n",
        "        text = soup.find_all(text=True) #finding out all the text from the soup\n",
        "        output = ''\n",
        "        outputSentences = []\n",
        "        '''\n",
        "        Cleaning the text received from the soup. Firstly we will remove all elements having their parents in blacklist \n",
        "        '''\n",
        "        for t in text:\n",
        "            if t.parent.name not in blacklist:\n",
        "                output += '{} '.format(t)\n",
        "                outputSentences.append('{} '.format(t))\n",
        "        outputSentences = [i.strip() for i in outputSentences]\n",
        "        # removing special characters\n",
        "        outputSentences = [re.sub('[^a-zA-Z0-9]+', ' ', _)\n",
        "                        for _ in outputSentences]\n",
        "        # removing all only digit phrases       \n",
        "        outputSentences = [' '.join(s for s in i.split() if not any(\n",
        "            c.isdigit() for c in s)) for i in outputSentences]\n",
        "        #removing empty sentences\n",
        "        outputSentences = [i for i in outputSentences if i]\n",
        "        #tokenising the sentence phrases using nltk\n",
        "        outputSentences = [nltk.tokenize.sent_tokenize(i) for i in outputSentences]\n",
        "        words = list(output.split(' '))\n",
        "        words = [re.sub('[^a-zA-Z0-9]+', ' ', _) for _ in words]\n",
        "        allWords = []\n",
        "        #Filtering words and removing words having length less than 2 and numbers\n",
        "        for word in words:\n",
        "            if len(word) > 2:\n",
        "                wordsInCurrent = word.split(' ')\n",
        "                for w in wordsInCurrent:\n",
        "                    if len(w) > 2 and not(w.isdecimal()):\n",
        "                        allWords.append(w.lower()) #appending lowercase words to the final list\n",
        "        return allWords, outputSentences\n",
        "    else:    \n",
        "        return [],[]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgFogx8fTyj6"
      },
      "source": [
        "'''\n",
        "The next two functions use multithreading for asynchronous crawling and gathering of data reveived by extraction of data from a list of URLs.\n",
        "These functions ensure that the crawling is done asynchronously and hence speeds up the process.\n",
        "'''\n",
        "async def threadPoolForGettingWordsAndLinks(URLs):\n",
        "    #using numpy arrays for faster append operation\n",
        "    wordsInCurrentLevel=np.array([])\n",
        "    sentenceInCurrentLevel=np.array([])\n",
        "    linksInNextLevel=np.array([])\n",
        "    with ThreadPoolExecutor(max_workers=len(URLs)) as executor:\n",
        "        with requests.Session() as session:\n",
        "            loop=asyncio.get_event_loop()\n",
        "            #the tasks to perform, here two tasks are performed in concurrency\n",
        "            tasks=[\n",
        "                [loop.run_in_executor(executor, getWordsAndSentencesFromLinkAndSession, *(link, session)) for link in URLs],\n",
        "                [loop.run_in_executor(executor, getLinksFromLinkAndSession, *(link, session)) for link in URLs]\n",
        "            ]\n",
        "        #appending data gathered from crawling to the numpy arrays            \n",
        "        for words, sentences in await asyncio.gather(*tasks[0]):\n",
        "            wordsInCurrentLevel=np.append(wordsInCurrentLevel, words)\n",
        "            sentenceInCurrentLevel=np.append(sentenceInCurrentLevel,sentences) \n",
        "        for links in await asyncio.gather(*tasks[1]):\n",
        "            linksInNextLevel=np.append(linksInNextLevel,links)    \n",
        "\n",
        "    return list(wordsInCurrentLevel),list(sentenceInCurrentLevel), list(linksInNextLevel)\n",
        "\n",
        "\n",
        "async def threadPoolForGettingWords(URLs):\n",
        "    #using numpy arrays for faster append operation\n",
        "    wordsInCurrentLevel=np.array([])\n",
        "    sentenceInCurrentLevel=np.array([])\n",
        "    #the threadPoolExecutor Service\n",
        "    with ThreadPoolExecutor(max_workers=max(1,len(URLs))) as executor:\n",
        "        with requests.Session() as session:\n",
        "            loop=asyncio.get_event_loop()\n",
        "            #the tasks to perform\n",
        "            tasks=[\n",
        "                [loop.run_in_executor(executor, getWordsAndSentencesFromLinkAndSession, *(link, session)) for link in URLs]\n",
        "                ]\n",
        "        #appending data gathered from crawling to the numpy arrays        \n",
        "        for words, sentences in await asyncio.gather(*tasks[0]):\n",
        "            wordsInCurrentLevel=np.append(wordsInCurrentLevel, words)\n",
        "            sentenceInCurrentLevel=np.append(sentenceInCurrentLevel,sentences)  \n",
        "\n",
        "    return list(wordsInCurrentLevel),list(sentenceInCurrentLevel) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywNEXHDkT1VV"
      },
      "source": [
        "'''\n",
        "This is the main function for starting the scrapping and crawling. The parameters it takes are the baseURL and the maximum depth it has to go till.\n",
        "Multithreading using asyncio and ThreadPoolExecutor for faster gathering of information during crawling.\n",
        "'''\n",
        "def startScraping(baseURL, maxLevels):\n",
        "    visited = {} # a visited dictionary to keep track of the visited links and also can be useful for counting how many times any URL occurs on a page\n",
        "    allURLs = [] #a list to maintain the URLs per level\n",
        "    allWords = [] #a list to maintain all the Words per level\n",
        "    allSentences = [] # a list to maintain all the sentences and phrases per level\n",
        "    \n",
        "    for level in range(0, maxLevels+1):\n",
        "        if level == 0:\n",
        "            visited[baseURL] = 1 #marking the URL visited so that it will not be visited again\n",
        "            l=[]\n",
        "            l.append(baseURL)\n",
        "            loop = asyncio.new_event_loop() #starting a new event loop\n",
        "            asyncio.set_event_loop(loop) #setting the loop\n",
        "            allURLs.append(l)\n",
        "            '''\n",
        "            If the current level(0) is the maxLevel then we don't need to search for URLs on that page.\n",
        "            Else we need to search for URLs on that page\n",
        "            '''\n",
        "            if level==maxLevels:\n",
        "                future=asyncio.ensure_future(threadPoolForGettingWords(l))\n",
        "                #running the process until all the data is gathered\n",
        "                wordsInCurrentLevel,sentences=loop.run_until_complete(future) \n",
        "\n",
        "                #adding to respective lists\n",
        "                allWords.append(wordsInCurrentLevel)\n",
        "                allSentences.append(sentences)\n",
        "            else:\n",
        "                future=asyncio.ensure_future(threadPoolForGettingWordsAndLinks(l))\n",
        "                #running the process until all the data is gathered\n",
        "                wordsInCurrentLevel,sentencesInCurrentLevel, LinksInNextLevel=loop.run_until_complete(future)\n",
        "\n",
        "                #adding to respective lists\n",
        "                allWords.append(wordsInCurrentLevel)\n",
        "                allSentences.append(sentencesInCurrentLevel)\n",
        "                allURLs.append(LinksInNextLevel)\n",
        "        \n",
        "        elif level == maxLevels:\n",
        "            #if the level is maxLevel we will not find the links on these pages and just find the words and sentences.\n",
        "            URLs=[] #list to hold valid URLs in current level\n",
        "            for link in allURLs[-1]:\n",
        "                if link not in visited.keys() and ((not link.startswith(\"mailto:\")) and (not (\"javascript:\" in link)) and (not link.endswith(\".png\")) and (not link.endswith(\".jpg\")) and (not link.endswith(\".jpeg\"))):\n",
        "                    URLs.append(link)\n",
        "                    visited[link]=1\n",
        "                else:\n",
        "                    if link in visited.keys(): #if the link is already visited increase the counter to know about duplicate URLs\n",
        "                        visited[link] += 1\n",
        "\n",
        "            loop = asyncio.new_event_loop()\n",
        "            asyncio.set_event_loop(loop)\n",
        "            future=asyncio.ensure_future(threadPoolForGettingWords(URLs)) #running the process until all the data is gathered\n",
        "            wordsInCurrentLevel,sentencesInCurrentLevel=loop.run_until_complete(future)    \n",
        "            \n",
        "            #adding to respective lists           \n",
        "            allWords.append(wordsInCurrentLevel)\n",
        "            allSentences.append(sentencesInCurrentLevel)\n",
        "        else:\n",
        "            URLs = [] #list to hold valid URLs in current level\n",
        "            for link in allURLs[-1]:\n",
        "                if link not in visited.keys() and ((not link.startswith(\"mailto:\")) and (not (\"javascript:\" in link)) and (not link.endswith(\".png\")) and (not link.endswith(\".jpg\")) and (not link.endswith(\".jpeg\"))):\n",
        "                    visited[link] = 1 #marking the URL visited so that it will not be visited again\n",
        "                    URLs.append(link)\n",
        "                else:\n",
        "                    if link in visited.keys():\n",
        "                        visited[link]+=1\n",
        "\n",
        "            loop = asyncio.new_event_loop()\n",
        "            asyncio.set_event_loop(loop)            \n",
        "            future=asyncio.ensure_future(threadPoolForGettingWordsAndLinks(URLs))\n",
        "            wordsInCurrentLevel,sentencesInCurrentLevel, URLsInNextLevel=loop.run_until_complete(future)      \n",
        "\n",
        "            #adding to respective lists\n",
        "            allURLs.append(URLsInNextLevel)\n",
        "            allWords.append(wordsInCurrentLevel)\n",
        "            allSentences.append(sentencesInCurrentLevel)\n",
        "    \n",
        "\n",
        "    #analyzing the data gotten from crawling\n",
        "    wordCloudWords, countOfWordsPerLevel, averageLengthOfWordsPerLevel = analyzeWords(\n",
        "        allWords)\n",
        "    #getting bigrams from sentences and phrases    \n",
        "    allBiagrams = getBigrams(allSentences)\n",
        "    wordCloud = {}\n",
        "    bigramCloud = {}\n",
        "    for key, val in wordCloudWords:\n",
        "        wordCloud[key]=val\n",
        "        # print(wordCloud)\n",
        "    for key, val in allBiagrams:\n",
        "        bigramCloud[key]=val\n",
        "    return (wordCloud, countOfWordsPerLevel,  averageLengthOfWordsPerLevel, bigramCloud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzdgpF7nJB3z"
      },
      "source": [
        "## Let's test an example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_TsfkKfJBbL"
      },
      "source": [
        "BASE_URL='https://my-web-spider.herokuapp.com/'\n",
        "MAX_LEVEL=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3obeYRTJ4tL",
        "outputId": "a1bbbf2a-c968-40d7-974a-0e9269eb9691"
      },
      "source": [
        "%timeit wordCloud, wordsInEachLevel, AvarageLengthOfWordsInEachLevel, bigramCloud = startScrapingWithoutMultithreading(BASE_URL, MAX_LEVEL-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 3: 59.9 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "sAU3vNj9VrSb",
        "outputId": "b845461c-29e8-470e-ac95-9cbdef9d8a0d"
      },
      "source": [
        " %timeit wordCloud, wordsInEachLevel, AvarageLengthOfWordsInEachLevel, bigramCloud = startScraping(BASE_URL, MAX_LEVEL-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-8c0f0b740bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit wordCloud, wordsInEachLevel, AvarageLengthOfWordsInEachLevel, bigramCloud = startScraping(BASE_URL, MAX_LEVEL-1)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m   1060\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m         \u001b[0mall_runs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1063\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-b244fed2295a>\u001b[0m in \u001b[0;36mstartScraping\u001b[0;34m(baseURL, maxLevels)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m#analyzing the data gotten from crawling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     wordCloudWords, countOfWordsPerLevel, averageLengthOfWordsPerLevel = analyzeWords(\n\u001b[0;32m---> 83\u001b[0;31m         allWords)\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;31m#getting bigrams from sentences and phrases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mallBiagrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetBigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallSentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-129d7f1555b5>\u001b[0m in \u001b[0;36manalyzeWords\u001b[0;34m(allWords)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwordsPerLevel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallWords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mcountOfWordsPerLevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Level \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordsPerLevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0maverageLengthOfWordsPerLevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Level \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordsPerLevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordsPerLevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         filtered_words = [\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFLJRixHT-XC"
      },
      "source": [
        "print(wordCloud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2Jn9OthK-QN"
      },
      "source": [
        "## Generating Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avq9F-PNLAOs"
      },
      "source": [
        "wordcloud = WordCloud(width=1000,height=500, max_words=1000000,relative_scaling=1,normalize_plurals=False).generate_from_frequencies(wordCloud)\n",
        "\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyxy7YZXPXkT"
      },
      "source": [
        "print(wordsInEachLevel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL4unsPaOCTZ"
      },
      "source": [
        "plt.bar(range(len(wordsInEachLevel)), list(wordsInEachLevel.values()), align='center')\n",
        "plt.xticks(range(len(wordsInEachLevel)), list(wordsInEachLevel.keys()))\n",
        "# # for python 2.x:\n",
        "# plt.bar(range(len(D)), D.values(), align='center')  # python 2.x\n",
        "# plt.xticks(range(len(D)), D.keys())  # in python 2.x\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERJ3Rr51Q7jb"
      },
      "source": [
        "print(AvarageLengthOfWordsInEachLevel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG-3apoDQYz5"
      },
      "source": [
        "plt.plot(list(AvarageLengthOfWordsInEachLevel.keys()), list(AvarageLengthOfWordsInEachLevel.values()), marker=\".\", markersize=40)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}